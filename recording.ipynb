{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KimW00Sung/fastapi-websocket-chat/blob/main/recording.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSvxDDROktpc",
        "outputId": "ba0d3876-79c3-47a1-f281-4533de21641c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting openai-whisper\n",
            "  Downloading openai-whisper-20240930.tar.gz (800 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (0.13.1)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting python-multipart\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.11.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.13.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.2.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.7.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.9.0)\n",
            "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (3.2.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile) (1.17.1)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.0)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.9.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->openai-whisper)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->openai-whisper)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyngrok-7.2.8-py3-none-any.whl (25 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m851.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803404 sha256=8e46c8d35b1721ba92ccde816b3def8d064e981659bb9314ebf8297adc58e733\n",
            "  Stored in directory: /root/.cache/pip/wheels/2f/f2/ce/6eb23db4091d026238ce76703bd66da60b969d70bcc81d5d3a\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: uvicorn, python-multipart, pyngrok, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, starlette, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, fastapi, openai-whisper\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed fastapi-0.115.12 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 openai-whisper-20240930 pyngrok-7.2.8 python-multipart-0.0.20 starlette-0.46.2 uvicorn-0.34.2\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi uvicorn openai-whisper soundfile pyngrok python-multipart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7tmerF7oIiJ",
        "outputId": "18c82fe5-2600-46ee-ff64-fac8de1d5de7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-27 14:41:45--  https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 13.248.244.96, 99.83.220.108, 35.71.179.82, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|13.248.244.96|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9395172 (9.0M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-v3-stable-linux-amd64.tgz.1’\n",
            "\n",
            "ngrok-v3-stable-lin 100%[===================>]   8.96M  15.3MB/s    in 0.6s    \n",
            "\n",
            "2025-05-27 14:41:46 (15.3 MB/s) - ‘ngrok-v3-stable-linux-amd64.tgz.1’ saved [9395172/9395172]\n",
            "\n",
            "ngrok\n"
          ]
        }
      ],
      "source": [
        "!wget https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz\n",
        "!tar -xvf ngrok-v3-stable-linux-amd64.tgz\n",
        "!chmod +x ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecCowD2YoKUr",
        "outputId": "b1f6c102-52dd-48e2-8c35-bb92c8ddd1e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "# 여기에 본인의 AUTH_TOKEN을 입력하세요\n",
        "AUTH_TOKEN = \"2xNmD31mGpn4V4cbqJF7sBEpiPY_6JbQxP2rMPfaS3XTUX1Pg\"\n",
        "!./ngrok config add-authtoken {AUTH_TOKEN}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGOGitF9oNmV",
        "outputId": "2989a91e-47f5-4f32-834f-d7786686b381"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "# FastAPI 서버 코드\n",
        "from fastapi import FastAPI, Request, File, UploadFile\n",
        "from fastapi.responses import HTMLResponse, JSONResponse\n",
        "from fastapi.staticfiles import StaticFiles\n",
        "from fastapi.templating import Jinja2Templates\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "import whisper\n",
        "import io\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "import uvicorn\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# CORS 설정 추가\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],  # 모든 도메인에서 접근 허용\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# 정적 파일 및 템플릿 설정\n",
        "app.mount(\"/static\", StaticFiles(directory=\"www\"), name=\"static\")\n",
        "templates = Jinja2Templates(directory=\"www\")\n",
        "\n",
        "# Whisper 모델 로드\n",
        "model = whisper.load_model(\"base\")\n",
        "\n",
        "@app.get(\"/\", response_class=HTMLResponse)\n",
        "async def index(request: Request):\n",
        "    return templates.TemplateResponse(\"index.html\", {\"request\": request})\n",
        "\n",
        "@app.post(\"/transcribe\")\n",
        "async def transcribe(file: UploadFile = File(...)):\n",
        "    # 파일 데이터를 메모리에 직접 로드\n",
        "    audio_data = await file.read()\n",
        "    audio_stream = io.BytesIO(audio_data)\n",
        "\n",
        "    try:\n",
        "        # 오디오 데이터를 NumPy 배열로 변환\n",
        "        audio_array, sample_rate = sf.read(audio_stream)\n",
        "\n",
        "        # 단일 채널로 변환 (Whisper 요구사항)\n",
        "        if len(audio_array.shape) > 1 and audio_array.shape[1] > 1:\n",
        "            audio_array = audio_array.mean(axis=1)\n",
        "\n",
        "        # Whisper 모델에 배열 직접 전달\n",
        "        result = model.transcribe(audio_array)\n",
        "\n",
        "        return JSONResponse(content={\"text\": result[\"text\"]})\n",
        "    except Exception as e:\n",
        "        print(f'Error: {e}')\n",
        "        return JSONResponse(content={\"error\": str(e)}, status_code=500)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=3000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "YTipen1YoP4r"
      },
      "outputs": [],
      "source": [
        "!mkdir -p www"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHT6AizJoRqS",
        "outputId": "b3a42e25-24da-4719-dcc2-4bbb33222dc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting www/index.html\n"
          ]
        }
      ],
      "source": [
        "%%writefile www/index.html\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"ko\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>Audio Recorder</title>\n",
        "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/css?family=Roboto:300,400,500,700&display=swap\"/>\n",
        "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\"/>\n",
        "    <script src=\"https://unpkg.com/react@18/umd/react.production.min.js\"></script>\n",
        "    <script src=\"https://unpkg.com/react-dom@18/umd/react-dom.production.min.js\"></script>\n",
        "    <script src=\"https://unpkg.com/@mui/material@5.14.5/umd/material-ui.production.min.js\"></script>\n",
        "    <script src=\"https://unpkg.com/@babel/standalone/babel.min.js\"></script>\n",
        "    <style>\n",
        "        body { margin: 0; font-family: 'Roboto', sans-serif; }\n",
        "        .container { max-width: 800px; margin: 0 auto; padding: 20px; }\n",
        "        .header { background-color: #1976d2; color: white; padding: 16px; margin-bottom: 20px; }\n",
        "        .chat-bubble { background-color: #f5f5f5; padding: 16px; margin-bottom: 8px; border-radius: 4px; }\n",
        "        .transcription-container { max-height: 300px; overflow-y: auto; margin-top: 16px; }\n",
        "        .button-container { display: flex; gap: 8px; margin-top: 16px; }\n",
        "        .url-input { width: 100%; margin-bottom: 16px; }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <div id=\"root\"></div>\n",
        "    <script type=\"text/babel\">\n",
        "        const { useState, useRef, useEffect } = React;\n",
        "        const {\n",
        "            AppBar, Toolbar, Typography, Button, Container, Box, Paper, TextField\n",
        "        } = MaterialUI;\n",
        "\n",
        "        const App = () => {\n",
        "            const [isRecording, setIsRecording] = useState(false);\n",
        "            const [transcriptions, setTranscriptions] = useState([]);\n",
        "            const [apiUrl, setApiUrl] = useState('');\n",
        "            const mediaRecorderRef = useRef(null);\n",
        "            const audioChunksRef = useRef([]);\n",
        "\n",
        "            useEffect(() => {\n",
        "                // 현재 URL에서 기본 API 주소 설정\n",
        "                const protocol = window.location.protocol;\n",
        "                const hostname = window.location.hostname;\n",
        "                const port = window.location.port;\n",
        "                const baseUrl = `${protocol}//${hostname}${port ? ':' + port : ''}`;\n",
        "                setApiUrl(`${baseUrl}/transcribe`);\n",
        "            }, []);\n",
        "\n",
        "            const handleApiUrlChange = (event) => {\n",
        "                setApiUrl(event.target.value);\n",
        "            };\n",
        "\n",
        "            const handleStartRecording = async () => {\n",
        "                try {\n",
        "                    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n",
        "                    mediaRecorderRef.current = new MediaRecorder(stream);\n",
        "\n",
        "                    mediaRecorderRef.current.ondataavailable = (event) => {\n",
        "                        audioChunksRef.current.push(event.data);\n",
        "                    };\n",
        "\n",
        "                    mediaRecorderRef.current.onstop = () => {\n",
        "                        sendAudioData();\n",
        "                    };\n",
        "\n",
        "                    audioChunksRef.current = [];\n",
        "                    mediaRecorderRef.current.start();\n",
        "                    setIsRecording(true);\n",
        "                } catch (error) {\n",
        "                    console.error('Error accessing microphone:', error);\n",
        "                    alert('마이크 접근에 실패했습니다. 마이크 권한을 허용해주세요.');\n",
        "                }\n",
        "            };\n",
        "\n",
        "            const handleStopRecording = () => {\n",
        "                if (mediaRecorderRef.current && isRecording) {\n",
        "                    mediaRecorderRef.current.stop();\n",
        "                    setIsRecording(false);\n",
        "                }\n",
        "            };\n",
        "\n",
        "            const sendAudioData = async () => {\n",
        "                const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/wav' });\n",
        "                const formData = new FormData();\n",
        "                formData.append('file', audioBlob, 'recording.wav');\n",
        "\n",
        "                try {\n",
        "                    const response = await fetch(apiUrl, {\n",
        "                        method: 'POST',\n",
        "                        body: formData,\n",
        "                    });\n",
        "\n",
        "                    const data = await response.json();\n",
        "                    if (data.text) {\n",
        "                        setTranscriptions((prev) => [...prev, data.text]);\n",
        "                    } else if (data.error) {\n",
        "                        setTranscriptions((prev) => [...prev, `Error: ${data.error}`]);\n",
        "                    }\n",
        "                } catch (error) {\n",
        "                    console.error('Error sending audio data:', error);\n",
        "                    setTranscriptions((prev) => [...prev, `Error: ${error.message}`]);\n",
        "                }\n",
        "            };\n",
        "\n",
        "            return (\n",
        "                <Container className=\"container\">\n",
        "                    <div className=\"header\">\n",
        "                        <Typography variant=\"h6\">Audio Recorder</Typography>\n",
        "                    </div>\n",
        "                    <Box>\n",
        "                        <TextField\n",
        "                            label=\"API URL\"\n",
        "                            variant=\"outlined\"\n",
        "                            fullWidth\n",
        "                            value={apiUrl}\n",
        "                            onChange={handleApiUrlChange}\n",
        "                            className=\"url-input\"\n",
        "                        />\n",
        "                        <div className=\"button-container\">\n",
        "                            <Button\n",
        "                                variant=\"contained\"\n",
        "                                color=\"primary\"\n",
        "                                onClick={handleStartRecording}\n",
        "                                disabled={isRecording}\n",
        "                            >\n",
        "                                Start Recording\n",
        "                            </Button>\n",
        "                            <Button\n",
        "                                variant=\"contained\"\n",
        "                                color=\"secondary\"\n",
        "                                onClick={handleStopRecording}\n",
        "                                disabled={!isRecording}\n",
        "                            >\n",
        "                                Stop Recording\n",
        "                            </Button>\n",
        "                        </div>\n",
        "                    </Box>\n",
        "                    <div className=\"transcription-container\">\n",
        "                        {transcriptions.map((text, index) => (\n",
        "                            <div key={index} className=\"chat-bubble\">\n",
        "                                {text}\n",
        "                            </div>\n",
        "                        ))}\n",
        "                    </div>\n",
        "                </Container>\n",
        "            );\n",
        "        };\n",
        "\n",
        "        ReactDOM.render(<App />, document.getElementById('root'));\n",
        "    </script>\n",
        "</body>\n",
        "</html>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1h00uerFoTpL",
        "outputId": "5c08c592-6169-4c70-e5de-5e9546335377"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting run_server.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile run_server.py\n",
        "import subprocess\n",
        "import time\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# FastAPI 서버 시작\n",
        "server_process = subprocess.Popen([\"python\", \"app.py\"])\n",
        "print(\"FastAPI 서버가 시작되었습니다.\")\n",
        "\n",
        "# ngrok 실행\n",
        "ngrok_process = subprocess.Popen([\"./ngrok\", \"http\", \"3000\"],\n",
        "                                stdout=subprocess.PIPE,\n",
        "                                stderr=subprocess.PIPE,\n",
        "                                universal_newlines=True)\n",
        "print(\"ngrok이 시작되었습니다.\")\n",
        "\n",
        "# ngrok URL 가져오기\n",
        "def get_ngrok_url():\n",
        "    # ngrok 상태 확인 API 호출\n",
        "    result = subprocess.run([\"curl\", \"http://localhost:4040/api/tunnels\"],\n",
        "                           capture_output=True,\n",
        "                           text=True)\n",
        "    if result.stdout:\n",
        "        import json\n",
        "        try:\n",
        "            tunnels = json.loads(result.stdout)['tunnels']\n",
        "            for tunnel in tunnels:\n",
        "                if tunnel['proto'] == 'https':\n",
        "                    return tunnel['public_url']\n",
        "        except Exception as e:\n",
        "            print(f\"ngrok URL 파싱 오류: {e}\")\n",
        "    return None\n",
        "\n",
        "# ngrok URL 출력\n",
        "print(\"ngrok 터널을 설정 중입니다...\")\n",
        "time.sleep(5)  # ngrok이 터널을 열 시간 제공\n",
        "\n",
        "ngrok_url = get_ngrok_url()\n",
        "if ngrok_url:\n",
        "    print(f\"ngrok 공개 URL: {ngrok_url}\")\n",
        "    print(f\"웹 인터페이스 접속 URL: {ngrok_url}\")\n",
        "    print(f\"API 엔드포인트 URL: {ngrok_url}/transcribe\")\n",
        "else:\n",
        "    print(\"ngrok URL을 찾을 수 없습니다. 터미널 출력을 확인하세요.\")\n",
        "\n",
        "try:\n",
        "    print(\"\\n서버를 종료하려면 Ctrl+C를 누르세요...\\n\")\n",
        "    while True:\n",
        "        time.sleep(1)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"서버를 종료합니다...\")\n",
        "    server_process.terminate()\n",
        "    ngrok_process.terminate()\n",
        "    print(\"서버가 종료되었습니다.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBYCW9VhoVnS",
        "outputId": "3d5d00ad-08e1-4a6e-c821-0cdea33a6537"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastAPI 서버가 시작되었습니다.\n",
            "ngrok이 시작되었습니다.\n",
            "ngrok 터널을 설정 중입니다...\n",
            "ngrok 공개 URL: https://e5b6-34-9-112-186.ngrok-free.app\n",
            "웹 인터페이스 접속 URL: https://e5b6-34-9-112-186.ngrok-free.app\n",
            "API 엔드포인트 URL: https://e5b6-34-9-112-186.ngrok-free.app/transcribe\n",
            "\n",
            "서버를 종료하려면 Ctrl+C를 누르세요...\n",
            "\n",
            "100%|████████████████████████████████████████| 139M/139M [00:01<00:00, 117MiB/s]\n",
            "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m3027\u001b[0m]\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
            "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
            "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:3000\u001b[0m (Press CTRL+C to quit)\n",
            "\u001b[32mINFO\u001b[0m:     175.120.109.101:0 - \"\u001b[1mGET / HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     175.120.109.101:0 - \"\u001b[1mGET /favicon.ico HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
            "서버를 종료합니다...\n",
            "서버가 종료되었습니다.\n",
            "Exception ignored in: <module 'threading' from '/usr/lib/python3.11/threading.py'>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1568, in _shutdown\n",
            "    assert tlock.locked()\n",
            "           ^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt: \n",
            "\u001b[32mINFO\u001b[0m:     Shutting down\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application shutdown.\n",
            "\u001b[32mINFO\u001b[0m:     Application shutdown complete.\n",
            "\u001b[32mINFO\u001b[0m:     Finished server process [\u001b[36m3027\u001b[0m]\n"
          ]
        }
      ],
      "source": [
        "!python run_server.py"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOEwJTNxICkDMlHkHn7QeII",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}